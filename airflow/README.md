# Airflow DAGs for CMMS System

Este directorio contiene los DAGs de Apache Airflow para automatizaciÃ³n del sistema CMMS.

## ğŸ“ Estructura

```
airflow/
â”œâ”€â”€ dags/
â”‚   â”œâ”€â”€ etl_ml_training_dag.py           # ETL y entrenamiento ML
â”‚   â”œâ”€â”€ preventive_maintenance_dag.py    # GeneraciÃ³n de mantenimiento preventivo
â”‚   â””â”€â”€ report_generation_dag.py         # GeneraciÃ³n de reportes
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ feature_engineering.py           # Script PySpark para ingenierÃ­a de caracterÃ­sticas
â”œâ”€â”€ airflow_variables.json               # Variables de configuraciÃ³n
â””â”€â”€ README.md                            # Este archivo
```

## ğŸš€ Despliegue en Cloud Composer

### 1. Crear Environment

```bash
gcloud composer environments create cmms-composer \
  --location us-central1 \
  --python-version 3 \
  --machine-type n1-standard-4 \
  --disk-size 30 \
  --node-count 3
```

### 2. Subir DAGs

```bash
# Subir todos los DAGs
gcloud composer environments storage dags import \
  --environment cmms-composer \
  --location us-central1 \
  --source dags/

# O subir uno por uno
gcloud composer environments storage dags import \
  --environment cmms-composer \
  --location us-central1 \
  --source dags/etl_ml_training_dag.py
```

### 3. Subir Scripts

```bash
# Crear bucket si no existe
gsutil mb gs://cmms-ml-data

# Subir script de PySpark
gsutil cp scripts/feature_engineering.py gs://cmms-ml-data/scripts/
```

### 4. Configurar Variables

```bash
# Importar todas las variables desde JSON
gcloud composer environments storage data import \
  --environment cmms-composer \
  --location us-central1 \
  --source airflow_variables.json

# O configurar una por una
gcloud composer environments run cmms-composer \
  --location us-central1 \
  variables set -- gcp_project_id your-project-id

gcloud composer environments run cmms-composer \
  --location us-central1 \
  variables set -- backend_api_url https://your-backend.run.app
```

### 5. Configurar Conexiones

```bash
# Cloud SQL PostgreSQL
gcloud composer environments run cmms-composer \
  --location us-central1 \
  connections add cloudsql_postgres \
  --conn-type postgres \
  --conn-host /cloudsql/project:region:instance \
  --conn-schema cmms_db \
  --conn-login postgres \
  --conn-password <password>
```

## ğŸ“‹ DAGs Disponibles

### 1. ETL and ML Training (`etl_ml_training`)

**Schedule:** Semanal (Domingos a las 2 AM)

**DescripciÃ³n:** Extrae datos de Cloud SQL, procesa caracterÃ­sticas con PySpark, y entrena el modelo ML.

**Tareas:**
1. ExtracciÃ³n de datos de activos
2. ExtracciÃ³n de datos de Ã³rdenes de trabajo
3. CreaciÃ³n de cluster Dataproc
4. IngenierÃ­a de caracterÃ­sticas (PySpark)
5. Entrenamiento del modelo
6. Despliegue a Vertex AI
7. EliminaciÃ³n del cluster
8. NotificaciÃ³n de Ã©xito

### 2. Preventive Maintenance (`preventive_maintenance_generator`)

**Schedule:** Diario (6 AM)

**DescripciÃ³n:** Genera Ã³rdenes de trabajo para planes de mantenimiento vencidos.

**Tareas:**
1. Consulta de planes vencidos
2. CreaciÃ³n de Ã³rdenes de trabajo
3. PublicaciÃ³n de notificaciones
4. EnvÃ­o de resumen por email

### 3. Report Generation (`weekly_kpi_report`)

**Schedule:** Semanal (Lunes a las 8 AM)

**DescripciÃ³n:** Genera y envÃ­a reportes semanales de KPIs.

**Tareas:**
1. ExtracciÃ³n de datos de KPIs
2. GeneraciÃ³n de grÃ¡ficos
3. GeneraciÃ³n de reporte PDF
4. Subida a Cloud Storage
5. EnvÃ­o por email

## âš™ï¸ ConfiguraciÃ³n

### Variables Requeridas

| Variable | DescripciÃ³n | Ejemplo |
|----------|-------------|---------|
| `gcp_project_id` | ID del proyecto GCP | `my-project-123` |
| `gcp_region` | RegiÃ³n de GCP | `us-central1` |
| `gcs_bucket_name` | Bucket para datos ML | `cmms-ml-data` |
| `backend_api_url` | URL del backend | `https://api.cmms.com` |
| `backend_api_token` | Token de autenticaciÃ³n | `Bearer token...` |
| `alert_email` | Emails para alertas | `admin@cmms.com` |
| `report_email` | Emails para reportes | `reports@cmms.com` |
| `sendgrid_api_key` | API key de SendGrid | `SG.xxx` |

### Conexiones Requeridas

| ConexiÃ³n | Tipo | DescripciÃ³n |
|----------|------|-------------|
| `cloudsql_postgres` | PostgreSQL | ConexiÃ³n a Cloud SQL |

## ğŸ” Monitoreo

### Acceder a Airflow UI

```bash
# Obtener URL del webserver
gcloud composer environments describe cmms-composer \
  --location us-central1 \
  --format="get(config.airflowUri)"
```

### Ver Logs

```bash
# Logs de un DAG especÃ­fico
gcloud composer environments run cmms-composer \
  --location us-central1 \
  dags list-runs -- -d etl_ml_training

# Logs de Cloud Logging
gcloud logging read "resource.type=cloud_composer_environment" \
  --limit 50
```

## ğŸ§ª Testing

### Test Local (Desarrollo)

```bash
# Instalar Airflow localmente
pip install apache-airflow

# Inicializar base de datos
airflow db init

# Crear usuario admin
airflow users create \
  --username admin \
  --firstname Admin \
  --lastname User \
  --role Admin \
  --email admin@example.com

# Iniciar webserver
airflow webserver --port 8080

# Iniciar scheduler (en otra terminal)
airflow scheduler
```

### Test de DAG

```bash
# Validar sintaxis del DAG
python dags/etl_ml_training_dag.py

# Test de una tarea especÃ­fica
airflow tasks test etl_ml_training extract_assets_data 2024-01-01
```

### Trigger Manual

```bash
# Desde CLI
gcloud composer environments run cmms-composer \
  --location us-central1 \
  dags trigger -- etl_ml_training

# Desde UI
# 1. Ir a Airflow UI
# 2. Click en el DAG
# 3. Click en "Trigger DAG"
```

## ğŸ“¦ Dependencias

### Python Packages

Los siguientes paquetes deben estar instalados en el environment de Composer:

```
apache-airflow-providers-google>=10.0.0
apache-airflow-providers-postgres>=5.0.0
pandas>=1.5.0
matplotlib>=3.5.0
seaborn>=0.12.0
requests>=2.28.0
```

### Instalar Dependencias

```bash
gcloud composer environments update cmms-composer \
  --location us-central1 \
  --update-pypi-packages-from-file requirements.txt
```

## ğŸ”’ Seguridad

### Service Account

El environment de Composer debe tener los siguientes permisos:

- `cloudsql.client` - Para conectar a Cloud SQL
- `storage.objectAdmin` - Para leer/escribir en GCS
- `dataproc.admin` - Para crear/eliminar clusters
- `aiplatform.user` - Para desplegar modelos en Vertex AI

### Secrets

Usar Secret Manager para credenciales sensibles:

```bash
# Crear secret
gcloud secrets create backend-api-token \
  --data-file=token.txt

# Dar acceso al service account
gcloud secrets add-iam-policy-binding backend-api-token \
  --member="serviceAccount:composer-sa@project.iam.gserviceaccount.com" \
  --role="roles/secretmanager.secretAccessor"
```

## ğŸ“Š Costos Estimados

### Cloud Composer

- Environment: ~$300/mes (n1-standard-4, 3 nodes)
- Storage: ~$5/mes

### Dataproc (por ejecuciÃ³n)

- Cluster (2 workers): ~$2/hora
- EjecuciÃ³n semanal: ~$8/mes

### Total Estimado: ~$313/mes

## ğŸ› Troubleshooting

### DAG no aparece en UI

1. Verificar sintaxis: `python dags/your_dag.py`
2. Revisar logs del scheduler
3. Verificar que el archivo estÃ© en la carpeta correcta

### Error de conexiÃ³n a Cloud SQL

1. Verificar que el proxy de Cloud SQL estÃ© habilitado
2. Revisar permisos del service account
3. Verificar formato de host: `/cloudsql/project:region:instance`

### Error en Dataproc

1. Verificar quotas de GCP
2. Revisar configuraciÃ³n del cluster
3. Verificar que el script estÃ© en GCS

### Error en Backend API

1. Verificar token de autenticaciÃ³n
2. Revisar URL del backend
3. Verificar que el backend estÃ© accesible desde Composer

## ğŸ“š Recursos

- [Cloud Composer Documentation](https://cloud.google.com/composer/docs)
- [Apache Airflow Documentation](https://airflow.apache.org/docs/)
- [Dataproc Documentation](https://cloud.google.com/dataproc/docs)
- [Vertex AI Documentation](https://cloud.google.com/vertex-ai/docs)

## ğŸ¤ Contribuir

Para agregar nuevos DAGs:

1. Crear archivo en `dags/`
2. Seguir convenciones de naming
3. Agregar documentaciÃ³n
4. Probar localmente
5. Subir a Composer
6. Actualizar este README
